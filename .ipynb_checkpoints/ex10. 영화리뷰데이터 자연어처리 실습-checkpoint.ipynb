{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50c0701",
   "metadata": {},
   "source": [
    "###  목표\n",
    "- IMDb 영화 리뷰 데이터 The Internet Movie Database의 약자\n",
    "- 자연어 전처리 과정을 실습\n",
    "- 영화리뷰데이터를 통한 감성분석 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b73f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\SMHRD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리\n",
    "import nltk\n",
    "nltk.download('all') # nltk에 있는 모든 라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "845c3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 토큰화 필요한 함수 호출\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 단어 빈도 계산에 사용할 Counter() 함수 호출\n",
    "from collections import Counter # 단어의 빈도수를 세어주는 함수\n",
    "from nltk.corpus import stopwords # 불용어 관련 함수\n",
    "from preprocessing import *\n",
    "# from preprocessing import pos_tagger -> 품사태깅 에 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95498a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jennifer Ehle was sparkling in \\\"Pride and Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review\n",
       "0           0  Watching Time Chasers, it obvious that it was ...\n",
       "1           1  I saw this film about 20 years ago and remembe...\n",
       "2           2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
       "3           3  I went to see this film with a great deal of e...\n",
       "4           4  Yes, I agree with everyone on this site this m...\n",
       "5           5  Jennifer Ehle was sparkling in \\\"Pride and Pre...\n",
       "6           6  Amy Poehler is a terrific comedian on Saturday...\n",
       "7           7  A plane carrying employees of a large biotech ...\n",
       "8           8  A well made, gritty science fiction movie, it ...\n",
       "9           9  Incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "# .tsv(tab으로구분된파일) -> delimiter(구분자) 속성을 \\t (tab)으로 지정\n",
    "df = pd.read_csv(\"imdb.tsv\", delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d71f9",
   "metadata": {},
   "source": [
    "### 대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f46aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자로 변환 -> .lower()\n",
    "# 중요 ) .lower : \"문자열 형태\" 만 소문자로 변환해줄수 있음\n",
    "# df['review']: (시리즈형태) -> 문자열 형태로 : .str\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d864f343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review\n",
       "0           0  watching time chasers, it obvious that it was ...\n",
       "1           1  i saw this film about 20 years ago and remembe...\n",
       "2           2  minor spoilers in new york, joan barnard (elvi...\n",
       "3           3  i went to see this film with a great deal of e...\n",
       "4           4  yes, i agree with everyone on this site this m...\n",
       "5           5  jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6           6  amy poehler is a terrific comedian on saturday...\n",
       "7           7  a plane carrying employees of a large biotech ...\n",
       "8           8  a well made, gritty science fiction movie, it ...\n",
       "9           9  incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60195e8e",
   "metadata": {},
   "source": [
    "### 단어토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470f9279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review                                        word_tokens\n",
       "0           0  watching time chasers, it obvious that it was ...  [watching, time, chasers, ,, it, obvious, that...\n",
       "1           1  i saw this film about 20 years ago and remembe...  [i, saw, this, film, about, 20, years, ago, an...\n",
       "2           2  minor spoilers in new york, joan barnard (elvi...  [minor, spoilers, in, new, york, ,, joan, barn...\n",
       "3           3  i went to see this film with a great deal of e...  [i, went, to, see, this, film, with, a, great,...\n",
       "4           4  yes, i agree with everyone on this site this m...  [yes, ,, i, agree, with, everyone, on, this, s...\n",
       "5           5  jennifer ehle was sparkling in \\\"pride and pre...  [jennifer, ehle, was, sparkling, in, \\, '', pr...\n",
       "6           6  amy poehler is a terrific comedian on saturday...  [amy, poehler, is, a, terrific, comedian, on, ...\n",
       "7           7  a plane carrying employees of a large biotech ...  [a, plane, carrying, employees, of, a, large, ...\n",
       "8           8  a well made, gritty science fiction movie, it ...  [a, well, made, ,, gritty, science, fiction, m...\n",
       "9           9  incredibly dumb and utterly predictable story ...  [incredibly, dumb, and, utterly, predictable, ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 코퍼스를 단어로 토큰화한 후 'word_tokens'라는 컬럼을 생성하기\n",
    "# word_tokenize 함수 사용\n",
    "# df : (데이터프레임형태) , df['review'] : (시리즈형태)\n",
    "# 중요 ) 데이터프레임 / 시리즈 형태에서 함수를 사용 -> .apply 메서드 사용!!\n",
    "## 1. df에서 review 컬럼 접근\n",
    "## 2. apply 메서드 사용해서 word_tokenize 함수 사용\n",
    "## 3. word_tokens라는 컬럼을 생성하여 값 담아주기\n",
    "df['word_tokens'] = df['review'].apply(word_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a910b1a",
   "metadata": {},
   "source": [
    "### 데이터 정제\n",
    "- 등장빈도, 단어길이, 불용어 세트를 사용하여 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c9313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영되도록 하는 함수\n",
    "## preprocessing.py를 중간에 수정했던 상황이였음\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb71ed",
   "metadata": {},
   "source": [
    "#### 람다함수 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "998cdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def plus(a,b) :\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9895d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, y)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 람다 함수 사용법\n",
    "lambda x, y : x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0811cc1",
   "metadata": {},
   "source": [
    "df['cleaned_words']=df['word_tokens'].apply(lambda x: clean_by_freq(x,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['cleaned_words']=df['word_tokens'].apply(lambda x: clean_by_freq(x,1)) \n",
    "# -> x에 df['word_tokens'] 의 행을 순서대로 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c344d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                      cleaned_words\n",
       "0           0  ...  [one, film, said, '', 's, really, bad, movie, ...\n",
       "1           1  ...                                       [film, film]\n",
       "2           2  ...  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3           3  ...  [went, film, film, 's, went, 's, jump, send, n...\n",
       "4           4  ...  [site, movie, bad, even, movie, made, 's, movi...\n",
       "5           5  ...  [ehle, '', '', northam, wonderful, '', '', won...\n",
       "6           6  ...  [role, movie, n't, 's, author, book, author, '...\n",
       "7           7  ...  [plane, --, ceo, 's, --, search, rescue, missi...\n",
       "8           8  ...  [gritty, movie, sci-fi, good, suspense, movie,...\n",
       "9           9  ...       [girl, girl, '', '', --, --, '', '', '', '']\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocession.py 에 정의된 함수\n",
    "## 빈도수에 따른 데이터 정제 함수 : clean_by_freq\n",
    "## 단어길이에 따른 데이터 정제 함수 : clean_by_len\n",
    "## 불용어 세트를 활용한 정제 함수 : clean_by_stopwords\n",
    "\n",
    "# 빈도수 정제 함수 사용 후 재대입\n",
    "df['cleaned_words']=df['word_tokens'].apply(lambda x: clean_by_freq(x,1)) \n",
    "# 단어길이(길이 = 2) 정제 함수 사용 후 재대입\n",
    "df['cleaned_words']=df['cleaned_words'].apply(lambda x:clean_by_len(x,2))\n",
    "# 불용어 세트를 활용한 정제 함수 사용 후 재대입\n",
    "stopwords_set = stopwords.words('english')\n",
    "df['cleaned_words']=df['cleaned_words'].apply(lambda x:clean_by_stopwords(x,stopwords_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce72d83",
   "metadata": {},
   "source": [
    "### 어간추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming_by_porter 함수 적용하기\n",
    "# cleaned_words 컬럼값에 함수 적용\n",
    "df['stemmed_tokens']=df['cleaned_words'].apply(stemming_by_porter) \n",
    "# 여기서는 들어가는 매개변수가 1개라서 lambda 안써도됨 알아서 행값이 하나씩 들어가기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edcfb071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>sent_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                        sent_tokens\n",
       "0           0  ...  [watching time chasers, it obvious that it was...\n",
       "1           1  ...  [i saw this film about 20 years ago and rememb...\n",
       "2           2  ...  [minor spoilers in new york, joan barnard (elv...\n",
       "3           3  ...  [i went to see this film with a great deal of ...\n",
       "4           4  ...  [yes, i agree with everyone on this site this ...\n",
       "5           5  ...  [jennifer ehle was sparkling in \\\"pride and pr...\n",
       "6           6  ...  [amy poehler is a terrific comedian on saturda...\n",
       "7           7  ...  [a plane carrying employees of a large biotech...\n",
       "8           8  ...  [a well made, gritty science fiction movie, it...\n",
       "9           9  ...  [incredibly dumb and utterly predictable story...\n",
       "\n",
       "[10 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장토큰화 진행\n",
    "# 항상 시리즈 / 데이터프레임 형태에 우리가 만든 함수 적용할때에는 .apply!\n",
    "# sent_tokenize 함수 적용\n",
    "# 'sent_tokens' 라는 컬럼 생성\n",
    "\n",
    "df['sent_tokens']=df['review'].apply(sent_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356a6a7",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38c882bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                  pos_tagged_tokens\n",
       "0           0  ...  [(watching, VBG), (time, NN), (chasers, NNS), ...\n",
       "1           1  ...  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...\n",
       "2           2  ...  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...\n",
       "3           3  ...  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...\n",
       "4           4  ...  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...\n",
       "5           5  ...  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...\n",
       "6           6  ...  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...\n",
       "7           7  ...  [(a, DT), (plane, NN), (carrying, VBG), (emplo...\n",
       "8           8  ...  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...\n",
       "9           9  ...  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos_tagger 함수 사용하여 'sent_tokens' 컬럼에 적용\n",
    "\n",
    "df['pos_tagged_tokens']=df['sent_tokens'].apply(pos_tagger)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d9dd9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('watching', 'VBG'),\n",
       " ('time', 'NN'),\n",
       " ('chasers', 'NNS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('obvious', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('bunch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('friends', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('maybe', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('around', 'IN'),\n",
       " ('one', 'CD'),\n",
       " ('day', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('film', 'NN'),\n",
       " ('school', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('\\\\', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " ('hey', 'NN'),\n",
       " (',', ','),\n",
       " ('let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('pool', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('money', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('!', '.'),\n",
       " ('\\\\', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('or', 'CC'),\n",
       " ('something', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.'),\n",
       " ('what', 'WP'),\n",
       " ('ever', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('still', 'RB'),\n",
       " ('ended', 'VBD'),\n",
       " ('up', 'RP'),\n",
       " ('making', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('--', ':'),\n",
       " ('dull', 'JJ'),\n",
       " ('story', 'NN'),\n",
       " (',', ','),\n",
       " ('bad', 'JJ'),\n",
       " ('script', 'NN'),\n",
       " (',', ','),\n",
       " ('lame', 'NN'),\n",
       " ('acting', 'NN'),\n",
       " (',', ','),\n",
       " ('poor', 'JJ'),\n",
       " ('cinematography', 'NN'),\n",
       " (',', ','),\n",
       " ('bottom', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('barrel', 'NN'),\n",
       " ('stock', 'NN'),\n",
       " ('music', 'NN'),\n",
       " (',', ','),\n",
       " ('etc', 'FW'),\n",
       " ('.', '.'),\n",
       " ('all', 'DT'),\n",
       " ('corners', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('cut', 'VBN'),\n",
       " (',', ','),\n",
       " ('except', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('one', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('would', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('prevented', 'VBN'),\n",
       " ('this', 'DT'),\n",
       " ('film', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('release', 'NN'),\n",
       " ('.', '.'),\n",
       " ('life', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_tagged_tokens'][0] # 각각의 단어가 무슨 품사인지 태그가 생김"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bde85a",
   "metadata": {},
   "source": [
    "### 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21c70e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watch, time, chaser, ,, it, obvious, that, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, year, ago, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoiler, in, new, york, ,, joan, barna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, go, to, see, this, film, with, a, great, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, be, sparkle, in, \\, '', pride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, be, a, terrific, comedian, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carry, employee, of, a, large, biot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, make, ,, gritty, science, fiction, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                  lemmatized_tokens\n",
       "0           0  ...  [watch, time, chaser, ,, it, obvious, that, it...\n",
       "1           1  ...  [i, saw, this, film, about, 20, year, ago, and...\n",
       "2           2  ...  [minor, spoiler, in, new, york, ,, joan, barna...\n",
       "3           3  ...  [i, go, to, see, this, film, with, a, great, d...\n",
       "4           4  ...  [yes, ,, i, agree, with, everyone, on, this, s...\n",
       "5           5  ...  [jennifer, ehle, be, sparkle, in, \\, '', pride...\n",
       "6           6  ...  [amy, poehler, be, a, terrific, comedian, on, ...\n",
       "7           7  ...  [a, plane, carry, employee, of, a, large, biot...\n",
       "8           8  ...  [a, well, make, ,, gritty, science, fiction, m...\n",
       "9           9  ...  [incredibly, dumb, and, utterly, predictable, ...\n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words_lemmatizer 사용\n",
    "# 'pos_tagged_tokens' 컬럼에서 표제어 추출하기\n",
    "# 'lemmatized_tokens' 라는 컬럼 생성\n",
    "\n",
    "df['lemmatized_tokens']=df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "df\n",
    "# watching -> watch ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cb0099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watch, time, chaser, ,, it, obvious, that, it...</td>\n",
       "      <td>[make, one, film, say, '', 's, make, really, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, year, ago, and...</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoiler, in, new, york, ,, joan, barna...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, go, to, see, this, film, with, a, great, d...</td>\n",
       "      <td>[go, film, film, 's, go, 's, jump, send, n't, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, movie, make, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, be, sparkle, in, \\, '', pride...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, be, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, funny, au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carry, employee, of, a, large, biot...</td>\n",
       "      <td>[plane, --, ceo, 's, --, go, search, rescue, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, make, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, movie, keep, sci-fi, good, kee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                     cleaned_tokens\n",
       "0           0  ...  [make, one, film, say, '', 's, make, really, b...\n",
       "1           1  ...                                       [film, film]\n",
       "2           2  ...  [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3           3  ...  [go, film, film, 's, go, 's, jump, send, n't, ...\n",
       "4           4  ...  [site, movie, bad, even, movie, movie, make, '...\n",
       "5           5  ...  [ehle, '', '', northam, wonderful, '', '', won...\n",
       "6           6  ...  [role, movie, n't, 's, author, book, funny, au...\n",
       "7           7  ...  [plane, --, ceo, 's, --, go, search, rescue, m...\n",
       "8           8  ...  [gritty, movie, movie, keep, sci-fi, good, kee...\n",
       "9           9  ...       [girl, girl, '', '', --, --, '', '', '', '']\n",
       "\n",
       "[10 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추가전처리\n",
    "\n",
    "# lemmatized_tokens 에 빈도수(1) (clean_by_freq), 단어길이(2)(clean_by_len), 불용어 정제(clean_by_stopwords)\n",
    "# 불용어 세트 = stopwords_set\n",
    "# cleaned_tokens라는 컬럼에 생성\n",
    "\n",
    "df['cleaned_tokens']=df['lemmatized_tokens'].apply(lambda x : clean_by_freq(x,1))\n",
    "df['cleaned_tokens']=df['cleaned_tokens'].apply(lambda x : clean_by_len(x,2))\n",
    "df['cleaned_tokens']=df['cleaned_tokens'].apply(lambda x : clean_by_stopwords(x,stopwords_set))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76367f13",
   "metadata": {},
   "source": [
    "### 정수 인코딩\n",
    "- 텍스트 데이터를 숫자 데이터로 변환\n",
    "- 등장 빈도를 기준으로 정렬한 다음 인덱스 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3d2d7",
   "metadata": {},
   "source": [
    "### 하나의 로우(행) 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bd0660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'movie', 'bad', 'even', 'movie', 'movie', 'make', \"'s\", 'movie', 'special', 'describe', 'movie', 'movie', 'describe', 'movie', 'jim', 'make', 'stand-up', 'day', 'stand-up', 'jim', 'like', 'jim', 'actor', 'love', 'stand', 'day', 'comedian', 'special', 'jim', 'day', 'even', 'site', 'love', 'jim', 'stand-up', 'jim', 'actor', 'movie', 'stand', 'comedian', 'jim', 'like', \"''\", \"'s\", \"''\", \"''\", 'really', \"''\", 'terrible', 'really', 'terrible', 'movie', 'terrible', 'really', 'bad', 'movie']\n"
     ]
    }
   ],
   "source": [
    "tokens = df['cleaned_tokens'][4]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7138970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 10), ('jim', 7), (\"''\", 4), ('stand-up', 3), ('day', 3), ('really', 3), ('terrible', 3), ('site', 2), ('bad', 2), ('even', 2), ('make', 2), (\"'s\", 2), ('special', 2), ('describe', 2), ('like', 2), ('actor', 2), ('love', 2), ('stand', 2), ('comedian', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도 계산 함수 -> Counter\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "# 단어 토큰들의 빈도수가 높은 순서대로 정렬\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print(vocab) # 리스트안에 튜플이 들어가 있는 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c7bfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 인덱스 부여\n",
    "word_to_idx = {}\n",
    "i = 0 \n",
    "for (word,frequency) in vocab:\n",
    "    i += 1\n",
    "    word_to_idx[word] = i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd76a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 1, 'jim': 2, \"''\": 3, 'stand-up': 4, 'day': 5, 'really': 6, 'terrible': 7, 'site': 8, 'bad': 9, 'even': 10, 'make': 11, \"'s\": 12, 'special': 13, 'describe': 14, 'like': 15, 'actor': 16, 'love': 17, 'stand': 18, 'comedian': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb056f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a0ac554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1, 9, 10, 1, 1, 11, 12, 1, 13, 14, 1, 1, 14, 1, 2, 11, 4, 5, 4, 2, 15, 2, 16, 17, 18, 5, 19, 13, 2, 5, 10, 8, 17, 2, 4, 2, 16, 1, 18, 19, 2, 15, 3, 12, 3, 3, 6, 3, 7, 6, 7, 1, 7, 6, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_idx = []\n",
    "\n",
    "for token in tokens:\n",
    "    idx = word_to_idx[token]\n",
    "    encoded_idx.append(idx)\n",
    "    \n",
    "print(encoded_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f34d3e",
   "metadata": {},
   "source": [
    "### 전체 데이터 프레임 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e5f0234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [make, one, film, say, '', 's, make, really, b...\n",
       "1                                         [film, film]\n",
       "2    [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3    [go, film, film, 's, go, 's, jump, send, n't, ...\n",
       "4    [site, movie, bad, even, movie, movie, make, '...\n",
       "5    [ehle, '', '', northam, wonderful, '', '', won...\n",
       "6    [role, movie, n't, 's, author, book, funny, au...\n",
       "7    [plane, --, ceo, 's, --, go, search, rescue, m...\n",
       "8    [gritty, movie, movie, keep, sci-fi, good, kee...\n",
       "9         [girl, girl, '', '', --, --, '', '', '', '']\n",
       "Name: cleaned_tokens, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens']"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0bc22e66",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28b099d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'one', 'film', 'say', \"''\", \"'s\", 'make', 'really', 'bad', 'movie', \"''\", 'like', 'say', 'make', 'really', 'bad', 'movie', 'bad', 'one', 'film', \"'s\", \"'s\", 'like', 'film', 'film', 'new', 'york', 'joan', 'barnard', 'elvire', 'audrey', 'barnard', 'john', 'saxon', 'italy', 'etruscan', 'tomb', 'joan', 'italy', 'colleague', 'italy', 'maggot', 'maggot', 'joan', 'drug', 'drug', 'tomb', 'colleague', 'story', 'end', 'new', 'york', 'joan', 'colleague', 'romantic', 'end', 'waste', 'time', 'watch', 'story', 'romantic', 'end', 'elvire', 'audrey', 'john', 'saxon', 'maggot', 'watch', 'etrusco', 'go', 'watch', 'go', 'go', 'waste', 'time', 'etrusco', 'etruscan', 'go', 'film', 'film', \"'s\", 'go', \"'s\", 'jump', 'send', \"n't\", 'jump', 'radio', \"n't\", 'send', 'reporter', 'fear', 'jump', 'fear', 'radio', 'reporter', \"n't\", 'radio', \"n't\", 'go', \"n't\", 'site', 'movie', 'bad', 'even', 'movie', 'movie', 'make', \"'s\", 'movie', 'special', 'describe', 'movie', 'movie', 'describe', 'movie', 'jim', 'make', 'stand-up', 'day', 'stand-up', 'jim', 'like', 'jim', 'actor', 'love', 'stand', 'day', 'comedian', 'special', 'jim', 'day', 'even', 'site', 'love', 'jim', 'stand-up', 'jim', 'actor', 'movie', 'stand', 'comedian', 'jim', 'like', \"''\", \"'s\", \"''\", \"''\", 'really', \"''\", 'terrible', 'really', 'terrible', 'movie', 'terrible', 'really', 'bad', 'movie', 'ehle', \"''\", \"''\", 'northam', 'wonderful', \"''\", \"''\", 'wonderful', 'ehle', 'northam', 'lust', 'lust', 'ehle', 'northam', 'role', 'movie', \"n't\", \"'s\", 'author', 'book', 'funny', 'author', \"'s\", 'author', 'role', \"n't\", 'funny', 'queen', 'corn', 'corn', 'queen', 'author', 'book', 'movie', \"n't\", 'plane', '--', 'ceo', \"'s\", '--', 'go', 'search', 'rescue', 'mission', 'call', 'ceo', 'harlan', 'knowles', 'lance', 'henriksen', 'put', 'search', 'rescue', 'mission', 'knowles', 'search', 'try', 'rescue', 'wood', 'film', 'one', 'lance', 'henriksen', 'one', 'two', 'could', 'easily', 'decent', 'film', 'two', \"'re\", 'quastel', \"'s\", '--', 'film', 'call', 'sasquatch', 'bad', 'edit', 'see', 'quastel', \"'s\", 'appear', \"''\", \"'s\", 'try', 'time', 'want', 'try', \"''\", 'potential', 'material', 'relate', 'plane', 'try', 'crib', 'material', 'relate', 'monster', 'crib', 'exposition', 'dialogue', 'potential', 'far', 'monster', 'costume', 'get', 'see', 'character', 'wood', 'could', 'quastel', 'would', 'stereotype', 'time', 'monster', \"'s\", \"''\", \"''\", 'edit', 'well', 'scene', 'decent', 'dialogue', 'could', 'easily', 'effective', 'sasquatch', 'make', 'reason', 'quastel', 'think', \"'s\", 'good', 'idea', 'dialogue', 'scene', 'occur', 'time', 'see', 'line', 'scene', 'line', 'scene', 'back', 'back', 'reason', 'think', \"'s\", 'good', 'idea', 'use', 'use', 'dialogue', 'whether', 'need', 'idea', 'time', 'irrelevant', 'comment', 'whether', 'irrelevant', 'comment', 'occur', 'one', 'time', 'reason', \"n't\", 'whether', 'scene', 'cut', 'random', 'scene', \"'re\", 'show', 'appear', 'random', 'important', 'either', 'never', 'appear', \"'re\", 'far', 'scene', 'reason', 'leave', 'scene', 'film', 'either', 'need', 'exposition', 'get', 'need', 'cut', \"'s\", 'important', 'monster', \"'s\", \"''\", \"''\", 'could', 'easily', 'show', 'reason', 'character', '--', '--', 'leave', 'even', 'though', \"'s\", 'reason', 'go', 'scene', 'even', 'though', 'never', 'reason', 'character', 'call', 'harlan', 'knowles', \"''\", \"''\", 'like', \"'re\", 'stereotype', 'reason', 'quastel', 'use', \"''\", \"''\", \"''\", \"''\", 'monster', 'scene', 'even', 'though', 'costume', \"n't\", 'bad', 'would', 'effective', 'put', 'bad', 'could', 'go', 'get', 'idea', 'want', 'like', 'film', 'good', \"'m\", 'henriksen', \"'m\", 'love', 'love', 'film', 'one', '--', 'could', \"n't\", 'time', 'think', \"''\", 'go', 'well', \"''\", 'quastel', 'make', 'gritty', 'movie', 'movie', 'keep', 'sci-fi', 'good', 'keep', 'suspense', 'look', 'movie', 'sci-fi', \"'re\", 'look', \"'re\", 'look', 'good', 'gritty', 'sci-fi', 'good', 'suspense', 'movie', 'good', 'girl', 'girl', \"''\", \"''\", '--', '--', \"''\", \"''\", \"''\", \"''\"]\n"
     ]
    }
   ],
   "source": [
    "print(sum(df['cleaned_tokens'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a0011b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 코퍼스의 토큰들을 전부 합하여 단어의 등장 빈도 계산\n",
    "word_to_idx = {}\n",
    "i = 0\n",
    "tokens = sum(df['cleaned_tokens'], [])\n",
    "\n",
    "# 등장 빈도 계산하기 - Counter 함수 \n",
    "vocab = Counter(tokens)\n",
    "\n",
    "# 등장 빈도 순서대로 정렬하기 - most_common()\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "for (word, frequency) in vocab :\n",
    "    i +=1\n",
    "    word_to_idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca6d89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 함수 정의\n",
    "def idx_encoder(tokens, word_to_idx) :\n",
    "    encoded_idx = [] # 인코딩한 값을 담아줄 리스트\n",
    "    \n",
    "    for token in tokens :\n",
    "        idx = word_to_idx[token]\n",
    "        encoded_idx.append(idx)\n",
    "        \n",
    "    return encoded_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5de0a2",
   "metadata": {},
   "source": [
    "### 전체 데이터 프레임 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09227a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [12, 15, 4, 58, 1, 2, 12, 20, 8, 3, 1, 16, 58,...\n",
       "1                                               [4, 4]\n",
       "2    [59, 60, 23, 61, 62, 63, 61, 64, 65, 29, 66, 6...\n",
       "3    [6, 4, 4, 2, 6, 2, 34, 73, 5, 34, 35, 5, 73, 7...\n",
       "4    [76, 3, 8, 21, 3, 3, 12, 2, 3, 77, 78, 3, 3, 7...\n",
       "5    [39, 1, 1, 40, 82, 1, 1, 82, 39, 40, 83, 83, 3...\n",
       "6    [84, 3, 5, 2, 25, 85, 86, 25, 2, 25, 84, 5, 86...\n",
       "7    [89, 10, 90, 2, 10, 6, 41, 42, 91, 43, 90, 92,...\n",
       "8    [125, 3, 3, 126, 56, 14, 126, 127, 57, 3, 56, ...\n",
       "9                 [128, 128, 1, 1, 10, 10, 1, 1, 1, 1]\n",
       "Name: cleaned_tokens, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens'].apply(lambda x : idx_encoder(x,word_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ae894",
   "metadata": {},
   "source": [
    "### 감성분석\n",
    "- 자연어에 담긴 어조가 긍정적인지, 부정적인지, 중립적인지 확인하는 작업\n",
    "- 축약형과 기호등을 고려하여 감성 지수를 추출\n",
    "- 소셜 미디어 텍스트를 분석할때 자주사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c737b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentimentNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "                                              0.0/126.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 126.0/126.0 kB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2023.5.7)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a47d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로딩\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer=SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d421c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n",
      "{'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'compound': -0.5255}\n",
      "{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n"
     ]
    }
   ],
   "source": [
    "text1 = \"This is a great movie!\"\n",
    "text2 = \"This is a terrible movie!\"\n",
    "text3 = \"This movie was just okay.\"\n",
    "\n",
    "# VADER 감성분석\n",
    "# 단어, 문장, 여러 문장으로 이루어진 코퍼스도 바로 감성 지수 계산가능\n",
    "# polarity_scores 메서드 사용 \n",
    "print(senti_analyzer.polarity_scores(text1))\n",
    "print(senti_analyzer.polarity_scores(text2))\n",
    "print(senti_analyzer.polarity_scores(text3))\n",
    "# neg : 부정 감성 지수\n",
    "# neu : 중립 감성 지수\n",
    "# pos : 긍정 감성 지수\n",
    "# compound : 위 세 지수를 적절하게 조합하여 -1 과 1 사이의 감성지수를 나타냄\n",
    "# -1에 가까우면 부정적인 의미. 1에 가까우면 긍정적인 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4646dc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    watching time chasers, it obvious that it was ...\n",
       "1    i saw this film about 20 years ago and remembe...\n",
       "2    minor spoilers in new york, joan barnard (elvi...\n",
       "3    i went to see this film with a great deal of e...\n",
       "4    yes, i agree with everyone on this site this m...\n",
       "5    jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6    amy poehler is a terrific comedian on saturday...\n",
       "7    a plane carrying employees of a large biotech ...\n",
       "8    a well made, gritty science fiction movie, it ...\n",
       "9    incredibly dumb and utterly predictable story ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79d5e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vader 감성분석 함수 정의\n",
    "def vader_sentiment(text):\n",
    "    senti_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # 감성 분석 - polarity_scores 메서드\n",
    "    senti_score = senti_analyzer.polarity_scores(text)['compound']\n",
    "    \n",
    "    return senti_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e32b21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>-0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>-0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>-0.2794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>-0.9707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>0.8444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>0.9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>0.8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>0.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>0.9887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>-0.7375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  vader_sentiment\n",
       "0  watching time chasers, it obvious that it was ...          -0.9095\n",
       "1  i saw this film about 20 years ago and remembe...          -0.9694\n",
       "2  minor spoilers in new york, joan barnard (elvi...          -0.2794\n",
       "3  i went to see this film with a great deal of e...          -0.9707\n",
       "4  yes, i agree with everyone on this site this m...           0.8444\n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...           0.9494\n",
       "6  amy poehler is a terrific comedian on saturday...           0.8473\n",
       "7  a plane carrying employees of a large biotech ...           0.9885\n",
       "8  a well made, gritty science fiction movie, it ...           0.9887\n",
       "9  incredibly dumb and utterly predictable story ...          -0.7375"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vader_sentiment']=df['review'].apply(vader_sentiment)\n",
    "df[['review', 'vader_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f71b12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3   -0.9707\n",
       "1   -0.9694\n",
       "0   -0.9095\n",
       "9   -0.7375\n",
       "2   -0.2794\n",
       "4    0.8444\n",
       "6    0.8473\n",
       "5    0.9494\n",
       "7    0.9885\n",
       "8    0.9887\n",
       "Name: vader_sentiment, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감성지수가 가장 낮은 리뷰는?\n",
    "df['vader_sentiment'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5adaf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i went to see this film with a great deal of excitement as i was at school with the director, he was even a good friend of mine for a while. but sorry mate, this film stinks. i can only talk about what was wrong with the first half because that's when i walked out and went to the pub for a much needed drink: 1) someone's standing on a balcony about to jump and so you send a helicopter to shine a searchlight on them??? i don't think so - nothing would make them more likely to jump. 2) local radio doesn't send reporters to cover people about to attempt suicide - again for fear of pressuring them into jumping - or for fear of encouraging copy-cat instances. 3) whatever the circumstances, radio reporters don't do live broadcasts from the 10th floor of a tower block. radio cars don't carry leads long enough to connect the microphone and headphones to the transmitter. 4) the stuck in the lift scene was utterly derivative 5) the acting and direction was almost non existent.i could go on, but i won't.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13f9059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a well made, gritty science fiction movie, it could be lost among hundreds of other similar movies, but it has several strong points to keep it near the top. for one, the writing and directing is very solid, and it manages for the most part to avoid many sci-fi cliches, though not all of them. it does a good job of keeping you in suspense, and the landscape and look of the movie will appeal to sci-fi fans. if you're looking for a masterpiece, this isn't it. but if you're looking for good old fashioned post-apoc, gritty future in space sci-fi, with good suspense and special effects, then this is the movie for you. thoroughly enjoyable, and a good ending.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edd82f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화의 감성 지수 총합은?\n",
    "df['vader_sentiment'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db1131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa0ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4117fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4565e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
